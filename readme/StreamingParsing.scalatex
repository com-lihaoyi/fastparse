@import Main._
@val tests = wd/'fastparse/'test/'src/'fastparse
@val main = wd/'fastparse/'src/'fastparse

@sect{Streaming Parsing}
    @p
        In addition to the parsing strings, you can also parse "streaming"
        data from @hl.scala{Iterator}s. To do so, simply pass in an
        @hl.scala{Iterator[String]} or @hl.scala{java.io.InputStream} instead
        of a @code{String} to the @hl.scala{fastparse.parse} method.

    @hl.ref(tests/"IteratorTests.scala", Seq("\"basic\"", ""))

    @p
        Streaming parsing still needs to buffer input in-memory: in particular,
        parsers like @sect.ref{Optional}, @sect.ref{Repeat} or
        @sect.ref{Either} means a parser may backtrack, and thus FastParse
        needs to buffer any input from where such a parsers starts parsing.
        Other parsers like @sect.ref{Capture} do not backtrack, but need
        to buffer data in order to return the data that gets captured. Using
        @sect.ref{Cuts} to prevent backtracking, apart from making
        @sect.ref{Debugging Parsers} easier, also allows FastParse to flush
        parts of the buffer that it no longer needs to backtrack into.

    @p
        @i
            In general every cut in your parser possibly reduces the memory
            used to buffer input for iterator parsing


    @sect{Streaming Parsing Buffer Size}
        @p
            This first benchmark measures the maximum size of buffered input
            when using streaming parsing, for some sample parsers we have in
            the test suite, for input-chunks of size 1 and 1024:

        @table(width := "100%", cls := "pure-table")
            @thead
                @th{Parser}
                @th{Maximum buffer @br for 1-sized chunk}
                @th{Maximum buffer @br for 1024-sized chunk}
                @th{Size of input}
                @th{Used memory}
            @tbody
                @tr
                    @td{ScalaParse}@td{1555}@td{2523}@td{147894}@td{1.4%}
                @tr
                    @td{PythonParse}@td{2006}@td{2867}@td{68558}@td{3.6%}

        @p
            As you can see, for these "typical" parsers, some input needs to
            be buffered to allow backtracking, but it turns out to be only a
            few percent of the total file size.
        @p
            These parsers make heavy use of backtracking operators like
            @sect.ref{Either} or @sect.ref{Repeat}, but also make
            heavy use of @sect.ref{Cuts}. This lets FastParse drop buffered
            input when it knows it can no longer backtrack.
        @p
            Another thing to note is the chunk size: a smaller chunk size
            reduces the granularity of the chunks that get buffered,
            reducing the buffer size. However, this comes at a performance
            cost, as you can see below...
        @p
            When parsing @code{Iterator[String]}s, the chunks-size is the size of
            each @code{String}. When parsing @code{java.io.InputStream}, the chunk
            size defaults to 4096 bytes, and can be configured by instatiating your own
            @code{fastparse.ParserInputSource.FromReadable}.

    @sect{Streaming Parsing Performance}
        @p
            This next benchmark measures the effect of streaming parsing on
            runtime performance, using two different chunk-sizes, compared
            to the performance of non-streaming parsing:

        @table(width := "100%", cls := "pure-table")
            @thead
                @th{Parser}
                @th{Score on the plain parsing}
                @th{Score on the iterator parsing @br for 1-sized chunk}
                @th{Score on the iterator parsing @br for 1024-sized chunk}
            @tbody
                @tr
                    @td{ScalaParse}@td{43}@td{33}@td{43}
                @tr
                    @td{PythonParse}@td{1150}@td{600}@td{890}

        @p
            Here, we can see that streaming parsing has a non-trivial effect
            on performance: ScalaParse seems unaffected by a chunks of size
            1024, and takes a 25% performance hit for chunks of size 1, but
            PythonParse takes a significant hit (25%, 47%). While smaller chunk sizes
            results in smaller buffers, it also comes with a performance
            cost. Exactly how big you want your input chunks to be is up to
            you to decide: FastParse will accept an iterator of chunks
            as large or as small as you want.

        @p
            In general, Streaming Parsing it always going to be a
            performance hit over parsing a single @code{String} you
            pre-loaded in memory. The point of streaming parsing is to
            handle cases where you can't/don't-want-to load everything in
            memory. In that case, if the choice is between slightly-slower
            parsing or an @code{OutOfMemory} error, streaming parsing is
            a good option to have.

    @sect{Streaming Parsing Limitations}
        @p
            Apart from the performance/memory tradeoff mentioned above,
            streaming parsing has some limitations that it is worth being aware of:
        @ul
            @li
                Performance of iterator parsing is always going to be
                slower than performance of raw @code{String}
                parsing: this is unavoidable given the overhead of maintaining
                and trimming the input buffer
            @li
                Memory use when parsing iterators is always going to depend
                on aggressive use of @sect.ref{Cuts} within the parser: most
                real-world parsers rely heavily on @sect.ref{Optional},
                @sect.ref{Repeat} and @sect.ref{Either}, all of which will
                cause input to be buffered in memory unless you use
                @sect.ref{Cuts} to avoid backtracking
            @li
                You can't use @sect.ref{Tracing} after parsing an iterator:
                tracing performs a second parse on the same input to generate
                its error information, and the iterator input gets exhausted after
                the first parsing pass and are not available for a second time.
            @li
                Streaming parsing does not support parsing
                @code{scala.Stream[String]}, as @code{scala.Stream} buffers
                everything in memory, making it pretty useless from a
                perspective of "streaming parsing" where you explicitly
                @i{don't} want to do that.
            @li
                Streaming parsing does not (and will likely never) support
                @lnk("\"async\" or \"push\" parsing", "http://stackoverflow.com/questions/15895124/what-is-push-approach-and-pull-approach-to-parsing").
                This is because FastParse's entire execution model is based
                on a straightforward recursive-descent over the input stream.
                It's unlikely we'll ever be able to graft async-parsing
                on top of this execution model.